# Memory-optimized training config for Google Colab (T4 GPU ~15GB)
# Usage: python scripts/train.py training=colab

optimizer:
  name: AdamW
  lr: 0.001
  weight_decay: 0.05  # Increased for regularization (V3.3)
  betas: [0.9, 0.999]

scheduler:
  name: CosineAnnealingLR
  T_max: 100
  eta_min: 1.0e-6

epochs: 100
gradient_clip: 1.0
spike_loss_weight: 5.0  # Increased to revive dead neurons (V3.4)

# Resume from checkpoint (set to path like "models/best_model.pt" to resume)
resume_from: null

# Early stopping
early_stopping:
  enabled: true
  patience: 15
  min_delta: 0.001
  monitor: val_loss

# Checkpointing
checkpoint:
  save_best: true
  save_last: true
  monitor: val_accuracy

# Logging
logging:
  log_every_n_steps: 10
  use_mlflow: true
