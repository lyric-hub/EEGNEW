# Training configuration (v3.5)

# Optimizer
optimizer:
  name: AdamW
  lr: 0.001
  weight_decay: 0.05
  betas: [0.9, 0.999]

# Learning rate scheduler
scheduler:
  name: CosineAnnealingLR
  T_max: 100
  eta_min: 1e-6

# Training parameters
epochs: 100
gradient_clip: 1.0
spike_loss_weight: 5.0      # Spike rate regularization

# Domain adaptation loss weights (v3.5)
domain_loss_weight: 0.1     # GRL adversarial loss
mmd_loss_weight: 0.1        # MMD distribution matching

# Early stopping
early_stopping:
  enabled: true
  patience: 15
  min_delta: 0.001
  monitor: val_loss

# Checkpointing
checkpoint:
  save_best: true
  save_last: true
  monitor: val_accuracy

# Logging
logging:
  log_every_n_steps: 10
  use_mlflow: true
